{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step segmentation model\n",
    "This notebook demonstrates the `MultiStepSegmentationModel` defined in `multistep_model.py`.\n",
    "Make sure `multistep_model.py` is in the same directory or your Python path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Model Instantiation\n",
    "First, we import the model and instantiate it with the necessary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating MultiStepSegmentationModel with:\n",
      "  img_channels=1, init_mask_channels=1, n_classes=1\n",
      "  cnn_features=4, D_orig=64, H_orig=128, W_orig=128\n",
      "  patch_d=16, patch_h=32, patch_w=32\n",
      "[BasePatchCNN init] In channels: 3, Hidden features: 4, Num classes: 1\n",
      "[CNNBlock init] In: 3, Out: 4\n",
      "[SUCCESS] Model instantiated successfully.\n"
     ]
    }
   ],
   "source": [
    "from multistep_model import MultiStepSegmentationModel\n",
    "import torch\n",
    "\n",
    "# Define model parameters (matching the example in multistep_model.py for consistency)\n",
    "img_channels = 1       # E.g., 1 for grayscale MRIs\n",
    "init_mask_channels = 1 # E.g., 1 for a binary initial estimate mask\n",
    "n_classes = 1          # E.g., 1 for binary segmentation (foreground vs background)\n",
    "cnn_features = 4     # Hidden features in the BasePatchCNN's CNNBlock\n",
    "D_orig, H_orig, W_orig = 64, 128, 128 # Original dimensions (Depth, Height, Width)\n",
    "\n",
    "# Patch dimensions are derived (D/4, H/4, W/4)\n",
    "p_d, p_h, p_w = D_orig//4, H_orig//4, W_orig//4\n",
    "\n",
    "print(f\"Instantiating MultiStepSegmentationModel with:\")\n",
    "print(f\"  img_channels={img_channels}, init_mask_channels={init_mask_channels}, n_classes={n_classes}\")\n",
    "print(f\"  cnn_features={cnn_features}, D_orig={D_orig}, H_orig={H_orig}, W_orig={W_orig}\")\n",
    "print(f\"  patch_d={p_d}, patch_h={p_h}, patch_w={p_w}\")\n",
    "\n",
    "try:\n",
    "    model = MultiStepSegmentationModel(\n",
    "        image_channels=img_channels, \n",
    "        initial_mask_channels=init_mask_channels,\n",
    "        num_classes=n_classes,\n",
    "        base_cnn_hidden_features=cnn_features,\n",
    "        patch_size_d=p_d, \n",
    "        patch_size_h=p_h, \n",
    "        patch_size_w=p_w,\n",
    "        verbose=False  # Set to True for detailed print output from the model, False for cleaner notebook\n",
    "    )\n",
    "    print(\"[SUCCESS] Model instantiated successfully.\")\n",
    "except TypeError as e:\n",
    "    print(f\"[ERROR] TypeError during model instantiation: {e}\")\n",
    "    print(f\"This likely means the __init__ method in multistep_model.py does not match the parameters being passed.\")\n",
    "    print(f\"Please ensure multistep_model.py is saved with the correct MultiStepSegmentationModel class definition and RESTART THE KERNEL.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] An unexpected error occurred during model instantiation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Test Forward Pass\n",
    "Next, we test the forward pass with dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing forward pass...\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([1, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([1, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([1, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([1, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([1, 1, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 2, 32, 64, 64]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=2, H_n=2, W_n=2\n",
      "[tensor_to_patches] Output patches shape: torch.Size([8, 2, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 1, 32, 64, 64]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=2, H_n=2, W_n=2\n",
      "[tensor_to_patches] Output patches shape: torch.Size([8, 1, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([8, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([8, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([8, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([8, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([8, 1, 16, 32, 32])\n",
      "[patches_to_tensor] Input patches shape: torch.Size([8, 1, 16, 32, 32]), Target B=1, Num_patch_dims=(2, 2, 2)\n",
      "[patches_to_tensor] Output tensor shape: torch.Size([1, 1, 32, 64, 64])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 2, 64, 128, 128]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=4, H_n=4, W_n=4\n",
      "[tensor_to_patches] Output patches shape: torch.Size([64, 2, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 1, 64, 128, 128]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=4, H_n=4, W_n=4\n",
      "[tensor_to_patches] Output patches shape: torch.Size([64, 1, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([64, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([64, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([64, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([64, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([64, 1, 16, 32, 32])\n",
      "[patches_to_tensor] Input patches shape: torch.Size([64, 1, 16, 32, 32]), Target B=1, Num_patch_dims=(4, 4, 4)\n",
      "[patches_to_tensor] Output tensor shape: torch.Size([1, 1, 64, 128, 128])\n",
      "[SUCCESS] Dummy forward pass successful.\n",
      "  Coarse logits shape: torch.Size([1, 1, 16, 32, 32])\n",
      "  Refine1 logits shape: torch.Size([1, 1, 32, 64, 64])\n",
      "  Final logits shape: torch.Size([1, 1, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "if 'model' in globals():\n",
    "    print(\"\\nTesting forward pass...\")\n",
    "    batch_size = 1\n",
    "    dummy_image = torch.randn(batch_size, img_channels, D_orig, H_orig, W_orig)\n",
    "    dummy_initial_mask = torch.randn(batch_size, init_mask_channels, D_orig, H_orig, W_orig)\n",
    "    \n",
    "    try:\n",
    "        # The model returns three sets of logits\n",
    "        logits_coarse, logits_refine1, logits_final = model(dummy_image, dummy_initial_mask)\n",
    "        print(f\"[SUCCESS] Dummy forward pass successful.\")\n",
    "        print(f\"  Coarse logits shape: {logits_coarse.shape}\")\n",
    "        print(f\"  Refine1 logits shape: {logits_refine1.shape}\")\n",
    "        print(f\"  Final logits shape: {logits_final.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error during dummy forward pass: {e}\")\n",
    "else:\n",
    "    print(\"Model not instantiated. Please run the previous cell successfully first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Install Visualization Libraries\n",
    "This cell installs `torchviz` and `graphviz` if they are not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing/checking torchviz and graphviz...\n",
      "Requirement already satisfied: torchviz in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: graphviz in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: torch in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torchviz) (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ralbe\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary libraries are installed for visualization\n",
    "print(\"\\nInstalling/checking torchviz and graphviz...\")\n",
    "!pip install torchviz graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Architecture Visualization\n",
    "This cell attempts to visualize the model architecture using `torchviz`.\n",
    "Graphviz also needs to be installed on your system (e.g., download from graphviz.org and add to PATH). \n",
    "If `dot.render` fails, it often means the Graphviz executables are not in your system PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting model visualization...\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([1, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([1, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([1, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([1, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([1, 1, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 2, 32, 64, 64]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=2, H_n=2, W_n=2\n",
      "[tensor_to_patches] Output patches shape: torch.Size([8, 2, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 1, 32, 64, 64]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=2, H_n=2, W_n=2\n",
      "[tensor_to_patches] Output patches shape: torch.Size([8, 1, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([8, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([8, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([8, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([8, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([8, 1, 16, 32, 32])\n",
      "[patches_to_tensor] Input patches shape: torch.Size([8, 1, 16, 32, 32]), Target B=1, Num_patch_dims=(2, 2, 2)\n",
      "[patches_to_tensor] Output tensor shape: torch.Size([1, 1, 32, 64, 64])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 2, 64, 128, 128]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=4, H_n=4, W_n=4\n",
      "[tensor_to_patches] Output patches shape: torch.Size([64, 2, 16, 32, 32])\n",
      "[tensor_to_patches] Input tensor shape: torch.Size([1, 1, 64, 128, 128]), Target patch dims: D=16, H=32, W=32\n",
      "[tensor_to_patches] Num patches: D_n=4, H_n=4, W_n=4\n",
      "[tensor_to_patches] Output patches shape: torch.Size([64, 1, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Input patch shape: torch.Size([64, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Input shape: torch.Size([64, 3, 16, 32, 32])\n",
      "  [CNNBlock fwd] Output shape: torch.Size([64, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Features after CNNBlock shape: torch.Size([64, 4, 16, 32, 32])\n",
      " [BasePatchCNN fwd] Output logits patch shape: torch.Size([64, 1, 16, 32, 32])\n",
      "[patches_to_tensor] Input patches shape: torch.Size([64, 1, 16, 32, 32]), Target B=1, Num_patch_dims=(4, 4, 4)\n",
      "[patches_to_tensor] Output tensor shape: torch.Size([1, 1, 64, 128, 128])\n",
      "Generating model graph... (this may take a moment)\n",
      "[SUCCESS] Model architecture graph saved to 'multistep_segmentation_model_architecture.png'.\n",
      "You can open this file to view the graph.\n"
     ]
    }
   ],
   "source": [
    "if 'model' in globals():\n",
    "    print(\"\\nAttempting model visualization...\")\n",
    "    try:\n",
    "        from torchviz import make_dot\n",
    "        \n",
    "        if not hasattr(model, 'verbose'):\n",
    "            print(\"[ERROR] The 'model' object is missing the 'verbose' attribute.\")\n",
    "            print(\"This indicates an issue with model instantiation. Ensure multistep_model.py is correct and the kernel was restarted after changes.\")\n",
    "        else:\n",
    "            # Create dummy inputs again for tracing\n",
    "            vis_dummy_image = torch.randn(1, img_channels, D_orig, H_orig, W_orig, requires_grad=True)\n",
    "            vis_dummy_initial_mask = torch.randn(1, init_mask_channels, D_orig, H_orig, W_orig, requires_grad=True)\n",
    "            \n",
    "            # Temporarily disable model's internal prints for cleaner viz\n",
    "            original_verbose_state = model.verbose\n",
    "            model.verbose = False \n",
    "            _, _, vis_output_final = model(vis_dummy_image, vis_dummy_initial_mask)\n",
    "            model.verbose = original_verbose_state # Restore\n",
    "            \n",
    "            params = {name: p for name, p in model.named_parameters() if p.requires_grad}\n",
    "            \n",
    "            print(\"Generating model graph... (this may take a moment)\" )\n",
    "            dot = make_dot(vis_output_final, params=params)\n",
    "            \n",
    "            file_path = \"multistep_segmentation_model_architecture\"\n",
    "            dot.render(file_path, format=\"png\")\n",
    "            print(f\"[SUCCESS] Model architecture graph saved to '{file_path}.png'.\")\n",
    "            print(f\"You can open this file to view the graph.\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"[ERROR] torchviz is not installed. Please run the pip install cell first.\")\n",
    "    except NameError as e:\n",
    "        print(f\"[ERROR] A variable might not be defined (e.g., model, img_channels): {e}. Ensure previous cells ran correctly.\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"[ERROR] AttributeError: {e}. This often means the model object is not what is expected or is missing an attribute.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An unexpected error occurred during visualization: {e}\")\n",
    "        print(\"This could be due to Graphviz not being installed or not found in the system PATH.\")\n",
    "else:\n",
    "    print(\"Model not instantiated. Please run the model instantiation cell successfully first before attempting visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
